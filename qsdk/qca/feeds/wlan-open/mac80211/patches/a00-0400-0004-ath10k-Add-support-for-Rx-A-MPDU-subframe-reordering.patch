--- a/drivers/net/wireless/ath/ath10k/core.h
+++ b/drivers/net/wireless/ath/ath10k/core.h
@@ -306,6 +306,11 @@ struct ath10k_dfs_stats {
 
 #define ATH10K_MAX_NUM_PEER_IDS (1 << 11) /* htt rx_desc limit */
 
+struct ath10k_tid_rx {
+	struct sk_buff_head entries[IEEE80211_MAX_AMPDU_BUF];
+	u8 window_sz;
+};
+
 struct ath10k_peer {
 	struct list_head list;
 	struct ieee80211_vif *vif;
@@ -317,6 +322,8 @@ struct ath10k_peer {
 
 	/* protected by ar->data_lock */
 	struct ieee80211_key_conf *keys[WMI_MAX_KEY_INDEX + 1];
+
+	struct ath10k_tid_rx *tid_rx[IEEE80211_NUM_TIDS];
 };
 
 struct ath10k_txq {
--- a/drivers/net/wireless/ath/ath10k/htt.h
+++ b/drivers/net/wireless/ath/ath10k/htt.h
@@ -31,6 +31,8 @@
 #include "rx_desc.h"
 #include "hw.h"
 
+struct ath10k_peer;
+
 enum htt_dbg_stats_type {
 	HTT_DBG_STATS_WAL_PDEV_TXRX	= 1 << 0,
 	HTT_DBG_STATS_RX_REORDER	= 1 << 1,
@@ -1811,5 +1813,11 @@ int ath10k_htt_tx(struct ath10k_htt *htt
 		  struct sk_buff *msdu);
 void ath10k_htt_rx_pktlog_completion_handler(struct ath10k *ar,
 					     struct sk_buff *skb);
+int ath10k_htt_rx_aggr_tid_init(struct ath10k_htt *htt,
+				struct ath10k_peer *peer,
+				u8 tid, u8 buf_size);
+void ath10k_htt_rx_aggr_tid_deinit(struct ath10k_htt *htt,
+				   struct ath10k_peer *peer,
+				   u8 tid);
 
 #endif
--- a/drivers/net/wireless/ath/ath10k/htt_rx.c
+++ b/drivers/net/wireless/ath/ath10k/htt_rx.c
@@ -1571,6 +1571,303 @@ static int ath10k_htt_rx_handle_amsdu(st
 	return 0;
 }
 
+static int ath10k_htt_rx_get_tot_mpdu_count(struct ath10k_htt *htt,
+					    struct htt_rx_indication *rx,
+					    int num_mpdu_ranges)
+{
+	struct htt_rx_indication_mpdu_range *mpdu_ranges;
+	int mpdu_count = 0;
+	int i;
+
+	mpdu_ranges = htt_rx_ind_get_mpdu_ranges(rx);
+
+	for (i = 0; i < num_mpdu_ranges; i++)
+		mpdu_count += mpdu_ranges[i].mpdu_count;
+
+	return mpdu_count;
+}
+
+static int ath10k_htt_rx_no_reorder(struct ath10k_htt *htt, int mpdu_count)
+{
+	int i;
+	int ret = 0;
+
+	for (i = 0; i < mpdu_count; i++) {
+		ret = ath10k_htt_rx_handle_amsdu(htt);
+		if (ret)
+			break;
+	}
+
+	return ret;
+}
+
+static int ath10k_htt_rx_reorder_store(struct ath10k_htt *htt, u16 peerid,
+				       u8 tid, u16 seq_no,
+				       struct sk_buff_head *rx_msdus)
+{
+	struct ath10k *ar = htt->ar;
+	struct ath10k_peer *peer;
+	struct ath10k_tid_rx *tid_rx;
+	u16 index;
+	int ret = 0;
+
+	spin_lock_bh(&ar->data_lock);
+
+	peer = ath10k_peer_find_by_id(ar, peerid);
+	if (!peer) {
+		ath10k_warn(ar, "no peer object found to store rx frame in reorder buffer peerid %hu\n",
+			    peerid);
+		ret = -EINVAL;
+		goto exit;
+	}
+
+	tid_rx = peer->tid_rx[tid];
+	if (!tid_rx) {
+		ath10k_dbg(ar, ATH10K_DBG_HTT,
+			   "not storing rx frame in reorder buf, no rx aggr enabled sta %pM tid %hu\n",
+			    peer->addr, tid);
+		/* TODO: This condition will be true for all rx frames for which
+		 * no aggregation session is negotiated. May be define different
+		 * enum of return status so that this will not look like an
+		 * error.
+		 */
+		ret = -EINVAL;
+		goto exit;
+	}
+
+	index = seq_no % tid_rx->window_sz;
+
+	if (!skb_queue_empty(&tid_rx->entries[index])) {
+		ath10k_dbg(ar, ATH10K_DBG_HTT,
+			   "frame already stored in reorder buf sta %pM tid %hu seq num %hu\n",
+			    peer->addr, tid, seq_no);
+		__skb_queue_purge(rx_msdus);
+		goto exit;
+	}
+
+	skb_queue_splice(rx_msdus, &tid_rx->entries[index]);
+
+exit:
+	spin_unlock_bh(&ar->data_lock);
+
+	return ret;
+}
+
+static void ath10k_htt_rx_reorder_flush(struct ath10k_htt *htt, u16 peerid,
+					u8 tid, u16 seq_start, u16 seq_end,
+					bool flush_all)
+{
+	struct ath10k *ar = htt->ar;
+	struct ath10k_peer *peer;
+	struct ath10k_tid_rx *tid_rx;
+	int i;
+
+	spin_lock_bh(&ar->data_lock);
+
+	peer = ath10k_peer_find_by_id(ar, peerid);
+	if (!peer) {
+		ath10k_warn(ar, "no peer object found to flush reorder rx frames peerid %hu\n",
+			    peerid);
+		goto exit;
+	}
+
+	tid_rx = peer->tid_rx[tid];
+	if (!tid_rx) {
+		ath10k_dbg(ar, ATH10K_DBG_HTT,
+			   "not flushing rx frame from reorder buf, no rx aggr enabled sta %pM tid %hu\n",
+			    peer->addr, tid);
+		goto exit;
+	}
+
+	if (flush_all) {
+		for (i = 0; i < IEEE80211_MAX_AMPDU_BUF; i++)
+			__skb_queue_purge(&tid_rx->entries[i]);
+		goto exit;
+	}
+
+	seq_start %= tid_rx->window_sz;
+	seq_end %= tid_rx->window_sz;
+
+	do {
+		__skb_queue_purge(&tid_rx->entries[seq_start]);
+		seq_start = (seq_start + 1) % tid_rx->window_sz;
+	} while (seq_start != seq_end);
+
+exit:
+	spin_unlock_bh(&ar->data_lock);
+}
+
+static void ath10k_htt_rx_reorder_flush_all(struct ath10k_htt *htt, u16 peerid)
+{
+	int i;
+
+	for (i = 0; i < IEEE80211_NUM_TIDS; i++)
+		ath10k_htt_rx_reorder_flush(htt, peerid, i, 0, 0, true);
+}
+
+static int ath10k_htt_rx_reorder_get_mpdus(struct ath10k_htt *htt, u16 peerid,
+					   u8 tid, u16 seq_start, u16 seq_end,
+					   struct sk_buff_head *mpdus)
+{
+	struct ath10k *ar = htt->ar;
+	struct ath10k_peer *peer;
+	struct ath10k_tid_rx *tid_rx;
+	struct sk_buff_head *reorder_entries;
+	int num_mpdus = 0;
+
+	spin_lock_bh(&ar->data_lock);
+
+	peer = ath10k_peer_find_by_id(ar, peerid);
+	if (!peer) {
+		ath10k_warn(ar, "no peer object found to release reorder rx frames %hu\n",
+			    peerid);
+		goto exit;
+	}
+
+	tid_rx = peer->tid_rx[tid];
+	if (!tid_rx) {
+		ath10k_dbg(ar, ATH10K_DBG_HTT,
+			   "not releasing rx frame from reorder buf, no rx aggr enabled sta %pM tid %hu\n",
+			    peer->addr, tid);
+		goto exit;
+	}
+
+	seq_start %= tid_rx->window_sz;
+	seq_end %= tid_rx->window_sz;
+
+	do {
+		reorder_entries = &tid_rx->entries[seq_start];
+		seq_start = (seq_start + 1) % tid_rx->window_sz;
+
+		if (skb_queue_empty(reorder_entries))
+			continue;
+
+		skb_queue_splice_init(reorder_entries, &mpdus[num_mpdus]);
+
+		num_mpdus++;
+	} while (seq_start != seq_end);
+
+exit:
+	spin_unlock_bh(&ar->data_lock);
+	return num_mpdus;
+}
+
+static void ath10k_htt_rx_reorder_rel(struct ath10k_htt *htt, u16 peerid,
+				      u8 tid, u16 seq_start, u16 seq_end)
+{
+	struct ath10k *ar = htt->ar;
+	static struct ieee80211_rx_status rx_status;
+	struct sk_buff_head *mpdus;
+	int num_mpdus;
+	int i;
+
+	mpdus = kcalloc(IEEE80211_MAX_AMPDU_BUF, sizeof(struct sk_buff_head),
+			GFP_ATOMIC);
+
+	if (WARN_ON(!mpdus))
+		return;
+
+	for (i = 0; i < IEEE80211_MAX_AMPDU_BUF; i++)
+		__skb_queue_head_init(&mpdus[i]);
+
+	num_mpdus = ath10k_htt_rx_reorder_get_mpdus(htt, peerid, tid, seq_start,
+						    seq_end, mpdus);
+
+	for (i = 0; i < num_mpdus; i++) {
+		ath10k_htt_rx_h_ppdu(ar, &mpdus[i], &rx_status, 0xffff);
+		ath10k_htt_rx_h_filter(ar, &mpdus[i], &rx_status);
+		ath10k_htt_rx_h_mpdu(ar, &mpdus[i], &rx_status);
+		ath10k_htt_rx_h_deliver(ar, &mpdus[i], &rx_status);
+	}
+
+	kfree(mpdus);
+}
+
+static int ath10k_htt_rx_proc_mpdu(struct ath10k_htt *htt, u16 peerid, u8 tid)
+{
+	struct ath10k *ar = htt->ar;
+	struct sk_buff_head amsdu;
+	struct sk_buff *msdu;
+	struct htt_rx_desc *rxd;
+	static struct ieee80211_rx_status rx_status;
+	int seq_num;
+	int ret;
+
+	/* TODO: Below logic is duplicated in more than one place, move it to
+	 * a function to avoid repetiion.
+	 */
+	__skb_queue_head_init(&amsdu);
+
+	spin_lock_bh(&htt->rx_ring.lock);
+	if (htt->rx_confused) {
+		spin_unlock_bh(&htt->rx_ring.lock);
+		return -EIO;
+	}
+	ret = ath10k_htt_rx_amsdu_pop(htt, &amsdu);
+	spin_unlock_bh(&htt->rx_ring.lock);
+
+	if (ret < 0) {
+		ath10k_warn(ar, "rx ring became corrupted: %d\n", ret);
+		__skb_queue_purge(&amsdu);
+		/* FIXME: It's probably a good idea to reboot the
+		 * device instead of leaving it inoperable.
+		 */
+		htt->rx_confused = true;
+		return ret;
+	}
+
+	msdu = skb_peek(&amsdu);
+	if (!msdu)
+		return 0;
+
+	rxd = (void *)msdu->data - sizeof(*rxd);
+	seq_num = MS(__le32_to_cpu(rxd->mpdu_start.info0),
+		     RX_MPDU_START_INFO0_SEQ_NUM);
+
+	ath10k_htt_rx_h_unchain(ar, &amsdu, ret > 0);
+
+	if (!ath10k_htt_rx_reorder_store(htt, peerid, tid, seq_num,
+					 &amsdu))
+		return 0;
+
+	/* Process and pass amsdu list to mac80211 when failed to store the
+	 * frames in reorder buffer because these may be non-aggregated frames.
+	 */
+
+	ath10k_htt_rx_h_ppdu(ar, &amsdu, &rx_status, 0xffff);
+	ath10k_htt_rx_h_filter(ar, &amsdu, &rx_status);
+	ath10k_htt_rx_h_mpdu(ar, &amsdu, &rx_status);
+	ath10k_htt_rx_h_deliver(ar, &amsdu, &rx_status);
+
+	return 0;
+}
+
+static int ath10k_htt_rx_proc_mpdu_range(struct ath10k_htt *htt, u16 peerid,
+					 u8 tid, int mpdu_count, int status)
+{
+	int i;
+	int ret = 0;
+
+	/* TODO: Check the the status for duplication. As per fw team,
+	 * HTT_RX_IND_MPDU_STATUS_ERR_DUP will be set for duplicated
+	 * aggregated frames. There are quite a lot of out-of-order
+	 * indication during iperf UDP traffic when duplication is not
+	 * handled in the driver.
+	 */
+	if (status != HTT_RX_IND_MPDU_STATUS_OK) {
+		ret = ath10k_htt_rx_no_reorder(htt, mpdu_count);
+		return ret;
+	}
+
+	for (i = 0; i < mpdu_count; i++) {
+		ret = ath10k_htt_rx_proc_mpdu(htt, peerid, tid);
+		if (ret)
+			break;
+	}
+
+	return ret;
+}
+
 static void ath10k_htt_rx_proc_rx_ind(struct ath10k_htt *htt,
 				      struct htt_rx_indication *rx)
 {
@@ -1578,22 +1875,78 @@ static void ath10k_htt_rx_proc_rx_ind(st
 	struct htt_rx_indication_mpdu_range *mpdu_ranges;
 	int num_mpdu_ranges;
 	int i, mpdu_count = 0;
-
+	int tot_mpdus;
+	u16 peerid;
+	u8 tid;
+	bool reorder_flush;
+	bool reorder_rel;
+	int flush_seq_start, flush_seq_end;
+	int rel_seq_start, rel_seq_end;
+	int status;
+	int ret = 0;
+	bool flush_all = false;
+
+	peerid = __le32_to_cpu(rx->hdr.peer_id);
+	tid = MS(rx->hdr.info0, HTT_RX_INDICATION_INFO0_EXT_TID);
+	flush_all = tid == HTT_DATA_TX_EXT_TID_INVALID;
+	tid &= IEEE80211_QOS_CTL_TID_MASK;
+	mpdu_ranges = htt_rx_ind_get_mpdu_ranges(rx);
 	num_mpdu_ranges = MS(__le32_to_cpu(rx->hdr.info1),
 			     HTT_RX_INDICATION_INFO1_NUM_MPDU_RANGES);
-	mpdu_ranges = htt_rx_ind_get_mpdu_ranges(rx);
+	reorder_flush = !!(rx->hdr.info0 & HTT_RX_INDICATION_INFO0_FLUSH_VALID);
+	reorder_rel = !!(rx->hdr.info0 & HTT_RX_INDICATION_INFO0_RELEASE_VALID);
 
-	ath10k_dbg_dump(ar, ATH10K_DBG_HTT_DUMP, NULL, "htt rx ind: ",
-			rx, sizeof(*rx) +
-			(sizeof(struct htt_rx_indication_mpdu_range) *
-				num_mpdu_ranges));
+	tot_mpdus = ath10k_htt_rx_get_tot_mpdu_count(htt, rx, num_mpdu_ranges);
 
-	for (i = 0; i < num_mpdu_ranges; i++)
-		mpdu_count += mpdu_ranges[i].mpdu_count;
+	if (peerid >= ATH10K_MAX_NUM_PEER_IDS) {
+		ath10k_htt_rx_no_reorder(htt, tot_mpdus);
+		return;
+	}
+
+	spin_lock_bh(&ar->data_lock);
 
-	atomic_add(mpdu_count, &htt->num_mpdus_ready);
+	if (!ar->peer_map[peerid]) {
+		spin_unlock_bh(&ar->data_lock);
+		ath10k_htt_rx_no_reorder(htt, tot_mpdus);
+		return;
+	}
 
-	tasklet_schedule(&htt->txrx_compl_task);
+	spin_unlock_bh(&ar->data_lock);
+
+	if (reorder_flush) {
+		if (flush_all) {
+			ath10k_htt_rx_reorder_flush_all(htt, peerid);
+		} else {
+			flush_seq_start =
+				MS(__le32_to_cpu(rx->hdr.info1),
+				   HTT_RX_INDICATION_INFO1_FLUSH_START_SEQNO);
+			flush_seq_end =
+				MS(__le32_to_cpu(rx->hdr.info1),
+				   HTT_RX_INDICATION_INFO1_FLUSH_END_SEQNO);
+			ath10k_htt_rx_reorder_flush(htt, peerid, tid,
+						    flush_seq_start,
+						    flush_seq_end, flush_all);
+		}
+	}
+
+	for (i = 0; i < num_mpdu_ranges; i++) {
+		mpdu_count = mpdu_ranges[i].mpdu_count;
+		status = mpdu_ranges[i].mpdu_range_status;
+
+		ret = ath10k_htt_rx_proc_mpdu_range(htt, peerid, tid,
+						    mpdu_count, status);
+		if (ret)
+			break;
+	}
+
+	if (ret || !reorder_rel)
+		return;
+
+	rel_seq_start = MS(__le32_to_cpu(rx->hdr.info1),
+			   HTT_RX_INDICATION_INFO1_RELEASE_START_SEQNO);
+	rel_seq_end = MS(__le32_to_cpu(rx->hdr.info1),
+			 HTT_RX_INDICATION_INFO1_RELEASE_END_SEQNO);
+	ath10k_htt_rx_reorder_rel(htt, peerid, tid, rel_seq_start, rel_seq_end);
 }
 
 static void ath10k_htt_rx_frag_handler(struct ath10k_htt *htt)
@@ -1654,6 +2007,64 @@ static void ath10k_htt_rx_tx_compl_ind(s
 	}
 }
 
+void ath10k_htt_rx_aggr_tid_deinit(struct ath10k_htt *htt,
+				   struct ath10k_peer *peer, u8 tid)
+{
+	struct ath10k *ar = htt->ar;
+	struct ath10k_tid_rx *tid_rx;
+	int i;
+
+	lockdep_assert_held(&ar->data_lock);
+
+	ath10k_dbg(ar, ATH10K_DBG_HTT,
+		   "htt rx stop rx ba session sta %pM tid %hu\n",
+		   peer->addr, tid);
+
+	tid_rx = peer->tid_rx[tid];
+
+	if (!tid_rx) {
+		ath10k_dbg(ar, ATH10K_DBG_HTT,
+			   "rx aggregation deinit called for uninitialized tid sta %pM tid %hhu\n",
+			   peer->addr, tid);
+		return;
+	}
+
+	for (i = 0; i < IEEE80211_MAX_AMPDU_BUF; i++)
+		__skb_queue_purge(&tid_rx->entries[i]);
+
+	kfree(tid_rx);
+	peer->tid_rx[tid] = NULL;
+}
+
+int ath10k_htt_rx_aggr_tid_init(struct ath10k_htt *htt,
+				struct ath10k_peer *peer,
+				u8 tid, u8 window_sz)
+{
+	struct ath10k *ar = htt->ar;
+	struct ath10k_tid_rx *tid_rx;
+	int i;
+
+	lockdep_assert_held(&ar->data_lock);
+
+	if (peer->tid_rx[tid]) {
+		ath10k_warn(ar, "rx aggregation is already started for this peer's tid sta %pM tid %hu\n",
+			    peer->addr, tid);
+		return -EINVAL;
+	}
+
+	tid_rx = kzalloc(sizeof(*tid_rx), GFP_ATOMIC);
+	if (!tid_rx)
+		return -ENOMEM;
+
+	tid_rx->window_sz = window_sz;
+	for (i = 0; i < IEEE80211_MAX_AMPDU_BUF; i++)
+		__skb_queue_head_init(&tid_rx->entries[i]);
+
+	peer->tid_rx[tid] = tid_rx;
+
+	return 0;
+}
+
 static void ath10k_htt_rx_addba(struct ath10k *ar, struct htt_resp *resp)
 {
 	struct htt_rx_addba *ev = &resp->rx_addba;
@@ -2234,9 +2645,16 @@ bool ath10k_htt_t2h_msg_handler(struct a
 		complete(&htt->target_version_received);
 		break;
 	}
-	case HTT_T2H_MSG_TYPE_RX_IND:
-		ath10k_htt_rx_proc_rx_ind(htt, &resp->rx_ind);
+	case HTT_T2H_MSG_TYPE_RX_IND: {
+		/* NOTE: With the current multiple tasklet design we may be
+		 * running into original issue of cpu getting hogged. When
+		 * tasklets are merged into one when napi support the following
+		 * will work fine in low speed platform as well.
+		 */
+		ath10k_htt_rx_proc_rx_ind(&ar->htt, &resp->rx_ind);
+		ath10k_htt_rx_msdu_buff_replenish(htt);
 		break;
+	}
 	case HTT_T2H_MSG_TYPE_PEER_MAP: {
 		struct htt_peer_map_event ev = {
 			.vdev_id = resp->peer_map.vdev_id,
@@ -2337,9 +2755,28 @@ bool ath10k_htt_t2h_msg_handler(struct a
 		break;
 	}
 	case HTT_T2H_MSG_TYPE_RX_FLUSH: {
-		/* Ignore this event because mac80211 takes care of Rx
-		 * aggregation reordering.
-		 */
+		struct htt_rx_flush *rx_flush = &resp->rx_flush;
+		u16 peerid;
+		u16 seq_start, seq_end;
+		u8 tid;
+		bool flush_all;
+
+		peerid = __le16_to_cpu(rx_flush->peer_id);
+		tid = rx_flush->tid & IEEE80211_QOS_CTL_TID_MASK;
+		flush_all = rx_flush->tid == HTT_DATA_TX_EXT_TID_INVALID;
+		seq_start = rx_flush->seq_num_start;
+		seq_end = rx_flush->seq_num_end;
+		if (rx_flush->mpdu_status == HTT_RX_FLUSH_MPDU_REORDER) {
+			ath10k_htt_rx_reorder_rel(&ar->htt, peerid, tid,
+						  seq_start, seq_end);
+		} else {
+			if (flush_all)
+				ath10k_htt_rx_reorder_flush_all(htt, peerid);
+			else
+				ath10k_htt_rx_reorder_flush(&ar->htt, peerid,
+							    tid, seq_start,
+							    seq_end, flush_all);
+		}
 		break;
 	}
 	case HTT_T2H_MSG_TYPE_RX_IN_ORD_PADDR_IND: {
--- a/drivers/net/wireless/ath/ath10k/mac.c
+++ b/drivers/net/wireless/ath/ath10k/mac.c
@@ -6998,18 +6998,46 @@ static int ath10k_ampdu_action(struct ie
 	struct ath10k_vif *arvif = ath10k_vif_to_arvif(vif);
 	struct ieee80211_sta *sta = params->sta;
 	enum ieee80211_ampdu_mlme_action action = params->action;
+	struct ath10k_peer *peer;
 	u16 tid = params->tid;
+	int ret = 0;
+
+	mutex_lock(&ar->conf_mutex);
 
 	ath10k_dbg(ar, ATH10K_DBG_MAC, "mac ampdu vdev_id %i sta %pM tid %hu action %d\n",
 		   arvif->vdev_id, sta->addr, tid, action);
 
 	switch (action) {
 	case IEEE80211_AMPDU_RX_START:
+		spin_lock_bh(&ar->data_lock);
+		peer = ath10k_peer_find(ar, arvif->vdev_id, sta->addr);
+		if (!peer) {
+			ath10k_warn(ar, "no valid peer found to start rx aggr sta %pM tid %hu\n",
+				    sta->addr, tid);
+			spin_unlock_bh(&ar->data_lock);
+			ret = -EINVAL;
+			break;
+		}
+
+		ret = ath10k_htt_rx_aggr_tid_init(&ar->htt, peer, tid,
+						  params->buf_size);
+		spin_unlock_bh(&ar->data_lock);
+		break;
 	case IEEE80211_AMPDU_RX_STOP:
-		/* HTT AddBa/DelBa events trigger mac80211 Rx BA session
-		 * creation/removal. Do we need to verify this?
-		 */
-		return 0;
+		spin_lock_bh(&ar->data_lock);
+		peer = ath10k_peer_find(ar, arvif->vdev_id, sta->addr);
+		if (!peer) {
+			ath10k_warn(ar, "no valid peer found to stop rx aggr sta %pM tid %hu\n",
+				    sta->addr, tid);
+			spin_unlock_bh(&ar->data_lock);
+			ret = -EINVAL;
+			break;
+		}
+
+		ath10k_htt_rx_aggr_tid_deinit(&ar->htt, peer, tid);
+		spin_unlock_bh(&ar->data_lock);
+		ret = 0;
+		break;
 	case IEEE80211_AMPDU_TX_START:
 	case IEEE80211_AMPDU_TX_STOP_CONT:
 	case IEEE80211_AMPDU_TX_STOP_FLUSH:
@@ -7018,10 +7046,15 @@ static int ath10k_ampdu_action(struct ie
 		/* Firmware offloads Tx aggregation entirely so deny mac80211
 		 * Tx aggregation requests.
 		 */
-		return -EOPNOTSUPP;
+		ret = -EOPNOTSUPP;
+		break;
+	default:
+		ret = -EINVAL;
 	}
 
-	return -EINVAL;
+	mutex_unlock(&ar->conf_mutex);
+
+	return ret;
 }
 
 static void
@@ -7902,6 +7935,7 @@ int ath10k_mac_register(struct ath10k *a
 	ieee80211_hw_set(ar->hw, WANT_MONITOR_VIF);
 	ieee80211_hw_set(ar->hw, CHANCTX_STA_CSA);
 	ieee80211_hw_set(ar->hw, QUEUE_CONTROL);
+	ieee80211_hw_set(ar->hw, SUPPORTS_REORDERING_BUFFER);
 
 	if (!test_bit(ATH10K_FLAG_RAW_MODE, &ar->dev_flags))
 		ieee80211_hw_set(ar->hw, SW_CRYPTO_CONTROL);
--- a/drivers/net/wireless/ath/ath10k/txrx.c
+++ b/drivers/net/wireless/ath/ath10k/txrx.c
@@ -227,6 +227,7 @@ void ath10k_peer_unmap_event(struct ath1
 {
 	struct ath10k *ar = htt->ar;
 	struct ath10k_peer *peer;
+	u8 i;
 
 	if (ev->peer_id >= ATH10K_MAX_NUM_PEER_IDS) {
 		ath10k_warn(ar,
@@ -246,6 +247,9 @@ void ath10k_peer_unmap_event(struct ath1
 	ath10k_dbg(ar, ATH10K_DBG_HTT, "htt peer unmap vdev %d peer %pM id %d\n",
 		   peer->vdev_id, peer->addr, ev->peer_id);
 
+	for (i = 0; i < IEEE80211_NUM_TIDS; i++)
+		ath10k_htt_rx_aggr_tid_deinit(&ar->htt, peer, i);
+
 	ar->peer_map[ev->peer_id] = NULL;
 	clear_bit(ev->peer_id, peer->peer_ids);
 
